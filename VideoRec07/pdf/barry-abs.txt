Comparison of ARTMAP Neural Networks for Classification for Face Recognition from Video

In video-based of face recognition applications, the What-and-Where Fusion Neural Network (WWFNN) has been shown to reduce the generalization error by accumulating a classifier’s predictions over time, according to each individual in the environment. In this paper, three ARTMAP variants – fuzzy ARTMAP, ART-EMAP (Stage 1) and ARTMAP-IC – are compared for the classification of faces detected in the WWFNN. ART-EMAP (stage 1) and ARTMAP-IC expand on the well-known fuzzy ARTMAP by using distributed activation of category neurons, and by biasing distributed predictions according to the number of time these neurons are activated by training set patterns. The average performance of the WWFNNs with each ARTMAP network is compared to the WWFNN with a reference k-NN classifier in terms of generalization error, convergence time and compression, using a data set of real-world video sequences. Simulations results indicate that when ARTMAP-IC is used inside the WWFNN, it can achieve a generalization error that is significantly higher (about 20% on average) than if fuzzy ARTMAP or ARTEMAP is used. Indeed, ARTMAP-IC is less discriminant than the two other ARTMAP networks in cases with complex decision bounderies, when the training data is limited and unbalanced, as found in complex video data. However, ARTMAP-IC can outperform the others when classes are designed with a larger number of training patterns.


old


In applications of face recognition from video, the What-and-Where fusion neural network has been shown to reduce the generalization error by effectively accumulating a classifier’s predictions over time, according to each individual in the environment. In this paper, fuzzy ARTMAP and ARTMAP-IC are compared for the classification of faces detected in video frames within the What-and-Where fusion neural network. ARTMAP refers to a family of neural network architectures that can perform fast on-line incremental learning to account for novelty encountered in the field, and process video streams at a high speeds, making them attractive for complex real-time applications. ARTMAP-IC expands on the well-known fuzzy ARTMAP by using a distributed activation of category neurons, and by biasing distributed predictions according to the number of times these neurons are activated by training set patterns. Average performance of What-and-Where fusion system is compared to that of the reference k-NN classifier in terms of generalization error, convergence time and compression, using a data set of real-world video sequences. Simulation results indicate that when ARTMAP-IC is used, it can achieve a recognition rate that is significantly lower (about 20%) than if fuzzy ARTMAP is used, and comparable to that of k-NN alone. Indeed, ARTMAP-IC is less effective than fuzzy ARTMAP with the complex decision boundaries and the limited training data found in complex video data. 


Videos: http://www.cvmt.dk/~tbm/Publications/2007/
Head direction dataset : http://www.cvmt.dk/projects/Hermes/head-data.html