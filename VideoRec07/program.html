<HTML>
<HEAD>

<TITLE>VideoRec'07: International workshop on Video Processing and Recognition</TITLE>
<!-- link rel="stylesheet" href="http://www.cv.iit.nrc.ca/pics/default.css"-->
<meta http-equiv="Content-Language" content="en-ca">
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<style>
<!-
a:active { font-weight: bold; text-decoration: underline }
A:visited { font-weight: bold; color:#993300; text-decoration: none;}
A:link { font-weight: bold; color:#12522E; text-decoration: none;}
A:hover { color:#12522E; text-decoration: underline;background-color:#cccccc;}
A:border { border: 1px solid black; }
P		{font-family: Verdana, Arial, Helvetica; font-size: 8pt;};
UL		{font-family: Verdana, Arial, Helvetica; font-size: 8pt;};
OL		{font-family: Verdana, Arial, Helvetica; font-size: 8pt;};
TD		{font-family: Verdana, Arial, Helvetica; font-size: 8pt;};
DIV		{font-family: Verdana, Arial, Helvetica; font-size: 8pt;};
PRE		{font-family: Times New Roman Cyr, Times New Roman; font-size: 9pt;};
BODY		{font-family: Verdana, Arial, Helvetica; font-size: 8pt;};
INPUT		{font-family: Verdana, Arial, Helvetica; font-size: 8pt;};
TEXTAREA	{font-family: Verdana, Arial, Helvetica; font-size: 8pt;};
->
</style>


</head>

<BODY text=#000000 vLink=#551a8b aLink=#ff0000 link=#0000ee bgColor=#00eeaa>
<!--BODY text=#000000 vLink=#551a8b aLink=#ff0000 link=#0000ee bgColor=#fffff4-->
<CENTER>


<TABLE cellPadding=5 width=800 bgColor=#FFFF99 noborder border="1">
  <TBODY>
  <TR>
    <TD colSpan=4 bgcolor="#FF3300">
    
    
<!---   header ------>    
      <TABLE cellSpacing=0 cellPadding=2 width="100%" border=0 bgcolor="#008080">
        <TBODY>
        <TR>
          <TD vAlign=middle align=center width=90 bgColor=#FF3300>
          <a href="http://www.visioninterface.net/fpiv04/preface.html#logo"><img border="0" src="http://www.computer-vision.org/VideoRec07/pics/fpiv-logo.gif" 
alt="Click here to read  
  about FPiV logo" width="90" height="90"></a> </TD>
          <TD noWrap align=middle bgColor=#FF3300 height="100">
            <p align="center"><i><b><u>VideoRec'07</u>: International
            workshop on&nbsp;</b><br>
            <font size="5"><b>
 Video&nbsp;Processing and Recognition</b></font><br>
            </i>May 28-30, 2007, Montreal, Canada<br>
            Marriott Chateau Champlain
            <BR><a href="http://www.computer-vision.org/VideoRec07">www.computer-vision.org/VideoRec07</a></p>
          </TD>
          <TD align=middle width=90 bgColor=#FF3300 nowrap>
            <p align="center"><a href="http://www.cipprs.org"><img border="0" src="http://www.computer-vision.org/VideoRec07/pics/cipprs2.gif" width="79" height="78"></a><a href="http://iit-iti.nrc-cnrc.gc.ca/r-d/cv-vi_e.html"><img  src="http://www.computer-vision.org/VideoRec07/pics/NRC-CNRC.gif" border="0" width="82" height="16"></a></p>
 </TD></TR>
             
        <!-- TR>
          <TD bgColor=#ff8888 colSpan=3>
            <p align="right"><font face="Times New Roman" size="6"><i><b>FPIV
            2004</b></i>&nbsp;</font></TD></TR-->
            </TBODY>
            </TABLE>
            </TD>
            </TR>
 
   <TR>
    <td align=center width="25%" bgcolor=#FF3300 height="25" nowrap><a href="index.html"><font size="1">introduction</font></a> </td>
    <TD noWrap align=center width="25%" bgcolor=#FF3300><a href="authors.html"><font size="1">submission
      instructions</font></a> 
 </TD>   
    <TD noWrap align=center width="25%" bgcolor=#FF3300><a href="program.html"><font size="1">workshop
      program</font></a> 
    </TD>
<TD noWrap align=center width="25%" bgcolor=#FF3300> <font size="1"> <a href="http://www.ift.ulaval.ca/ai06/">AI'07</a>
  / <a href="http://www.cs.usask.ca/~gutwin/gi/">GI'07</a> / <a href="http://www.computerrobotvision.org/">CRV'07</a>
  / <a href="http://www.aigicrvis.ca/home/index.html">IS'07</a></font>



 </TD>
      </TR>
      
 <!-- End of header ----> 
      
  <TR>
    <TD colSpan=4> 

    
        <h3 align="center">Workshop Abstracts</h3>
        <p align="center"><b><font size="2">VideoRec'07 workshop was held as a Special
        Session of the CRV'07 conference<br>
        (For entire CRV'07 program&nbsp; click <a href="http://www.aigicrvis.ca/program/CRV2007/index.html">here</a>.
        For all joint conferences click <a href="http://www.aigicrvis.ca/program/index.html">here</a>)</font></b></p>
        <p align="center"><b><font size="3">Oral session*</font><font size="2"><br>
        </font>Tuesday 15:30 - 18:30&nbsp;(Maisonneuve - 36th Floor)<br>
        (Full papers can be downloaded from <a href="http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/proceedings/crv/&amp;toc=comp/proceedings/crv/2007/2786/00/2786toc.xml">ieeexplore.ieee.org</a>)
        </b></p>
    </center><font SIZE="2">
    <ul>
      <li>
        <p align="left">15:30&nbsp;<br>
          Vision-Based Motorcycle Detection and Tracking System with Occlusion
          Segmentation&nbsp;<br>
          Chung-Cheng Chiu, Min-Yu Ku, Hung-Tsung Chen<br>
          <br>
      </li>
        <li>
          <p align="left">15:50&nbsp;<br>
          Real-time commercial recognition Using Color Moments and Hashing&nbsp;<br>
          Abhishek Shivadas, John M. Gauch&nbsp;<br>
          <br>
          Abstract: In this paper, our focus is on real-time commercial recognition. In particular, our goal is to correctly identify all commercials that are stored in our commercial database within the first second of their broadcast. To meet this objective, we make use of 27 color moments to characterize the content of every video frame. This representation is much more compact than most color histogram representations, and it less sensitive to noise and other distortion. We use framelevel hashing with subsequent matching of moment vectors and video frames to perform commercial recognition. Hashing provides constant time access to millions of video frames, so this approach can perform in real-time for databases containing thousands of commercials. In our experiments with a database of 63 commercials, we achieved 96% recall, 100% precision, and 98% utility while recognizing commercials within the first 1/2 second of their broadcast.&nbsp;<br>
          <br>
        </li>
        <li>
          <p align="left">16:10&nbsp;<br>
          Constructing Face Image Logs that are Both Complete and Concise&nbsp;<br>
          Adam Fourney and Robert Laganière<br>
          <br>
          Abstract: This paper describes a construct that we call a face image
          log. Face image logs are collections of time stamped images
          representing faces detected in surveillance videos. The techniques
          demonstrated in this paper strive to construct face image logs that
          are complete and concise in the sense that the logs contain only the
          best images available for each individual observed. We begin by
          describing how to assess and compare the quality of face images. We
          then illustrate a robust method for selecting high quality images.
          This selection process takes into consideration the limitations
          inherent in existing face detection and person tracking techniques.
          Experimental results demonstrate that face logs constructed in this
          manner generally contain fewer than 5% of all detected faces, yet
          these faces are of high quality, and they represent all individuals
          detected in the video sequence.<br>
          <br>
        </li>
        <li>
          <p align="left">16:30&nbsp;<br>
          Real-time eye blink detection with GPU-based SIFT tracking<br>
          Marc Lalonde, David Byrns, Langis Gagnon, Normand Teasdale, Denis Laurendeau<br>
          <br>
    </font>
          Abstract: This paper reports on the implementation of a GPU-based,
          real-time eye blink detector on very low contrast images
          acquired under near-infrared illumination. This detector
          is part of a multi-sensor data acquisition and analysis
          system for driver performance assessment and training.<br>
          Eye blinks are detected inside regions of interest that are
          aligned with the subject’s eyes at initialization. Alignment
          is maintained through time by tracking SIFT feature points that are used to estimate the affine transformation between
          the initial face pose and the pose in subsequent frames. The
          GPU implementation of the SIFT feature point extraction
          algorithm ensures real-time processing. An eye blink detection
          rate of 97% is obtained on a video dataset of 33,000
          frames showing 237 blinks from 22 subjects.<font SIZE="2">
          <br>
          <br>
        </li>
        <li>
          <p align="left">16 :50&nbsp;<br>
          A Robust Video Foreground Segmentation by Using Generalized Gaussian
          Mixture Modelling&nbsp;<br>
          Mohand Saïd Allili, Nizar Bouguila and Djemel Ziou&nbsp;<br>
          <br>
          Abstract: In this paper, we propose a novel object tracking algorithm in video sequences. The formulation of the object tracking is based on variational calculus, where an adaptive parametric mixture model is used for object features representation. The tracking is based on matching the object mixture models between successive frames of the sequence by using active contours while adapting the mixture model to varying object appearance changes due to illumination conditions and camera geometry. The implementation of the method is based on level set active contours which allow for automatic topology changes and stable numerical schemes. We validate our approach on examples of object tracking performed on real video sequences.&nbsp;<br>
          <br>
        </li>
        <li>
          <p align="left">17:10&nbsp;<br>
          A Framework for 3D Hand Tracking and Gesture Recognition&nbsp;<br>
          <a href="http://www.discover.uottawa.ca/~aelsawah/research.html">
          Ayman El-Sawah</a>, Chris Joslin, Nicolas D. Georganas, Emil M. Petriu<br>
          <br>
          Abstract: In this paper we present a framework for 3D hand
          tracking and dynamic gesture recognition using a single camera. Hand
          tracking is performed in a two step process: we first generate 3D hand
          posture hypothesis using geometric and kinematics inverse
          transformations, and then validate the hypothesis by projecting the
          postures on the image plane and comparing the projected model with the
          ground truth using a probabilistic observation model. Dynamic gesture
          recognition is performed using a Dynamic Bayesian Network model. The
          framework utilizes elements of soft computing to resolve the ambiguity
          inherent in vision-based tracking by producing a fuzzy hand posture
          output by the hand tracking module and feeding back potential posture
          hypothesis from the gesture recognition module.<br>
          <br>
        </li>
        <li>
          <p align="left">17:30&nbsp;<br>
          Adaptive Appearance Model for Object Contour Tracking in Videos<br>
          Mohand Saïd Allili and Djemel Ziou<br>
          <br>
          Abstract: In this paper, we propose a novel object tracking algorithm in video sequences. The formulation of the object tracking is based on variational calculus, where an adaptive parametric mixture model is used for object features representation. The tracking is based on matching the object mixture models between successive frames of the sequence by using active contours while adapting the mixture model to varying object appearance changes due to illumination conditions and camera geometry. The implementation of the method is based on level set active contours which allow for automatic topology changes and stable numerical schemes. We validate our approach on examples of object tracking performed on real video sequences.&nbsp;<br>
          <br>
        </li>
        <li>
          <p align="left">17:50&nbsp;<br>
          Automatic Annotation of Humans in Surveillance Video<br>
          T.B. Moeslund, D.M. Hansen, P.Y. Duizer<br>
          <br>
 Abstract: In this paper we present a system for automatic annotation of humans
          passing a surveillance camera. Each human has 4 associated
          annotations: the primary color of the clothing, the height, and focus
          of attention. The annotation occurs after robust background
          subtraction based on a Codebook representation. The primary colors of
          the clothing are estimated by grouping similar pixels according to a
          body model. The height is estimated based on a 3D mapping using the
          head and feet. Lastly, the focus of attention is defined as the
          overall direction of the head, which is estimated using changes in
          intensity at four different positions. Results show successful
          detection and hence successful annotation for most test sequences.<br>
          <br>
          [<a href="http://www.cvmt.dk/~tbm/Publications/2007/">Video</a>] [<a href="http://www.cvmt.dk/projects/Hermes/head-data.html">Head direction dataset</a>]<br>
          <br>
        </li>
        <li>
          <p align="left">18:10&nbsp;<br>
          Registration of IR and Video Sequences Based on Frame Difference<br>
          Zheng Liu and Robert Laganière<br>
          <br>
          Abstract: Multi-modal imaging sensors are employed in advanced
          surveillance systems in the recent years. The performance of
          surveillance systems can be enhanced by using information beyond the
          visible spectrum, for example, infrared imaging. To ensure correctness
          of low- or high-level processing, multi-modal imagers must be fully
          calibrated or registered. In this paper, an algorithm is proposed to
          register the video sequences acquired by an infrared and an
          electro-optical (CCD) camera. The registration method is based on the
          silhouette extracted by differencing adjacent frames. This difference
          is found by an image structural similarity measurement. Initial
          registration is implemented by tracing the top head points in
          consecutive frames. Finally, an optimization procedure to maximize
          mutual information is employed to refine the registration results.</li>
    </ul>
    </font>
<CENTER>


<p align="center">&nbsp;</p>
<p align="center"><b><font size="3">Poster/Demo Session**<br>
</font>Held together with CRV'07 Poster Session 2&nbsp;<br>
Tuesday 13:00 - 15:00&nbsp; (Maisonneuve - 36th Floor)<br>
(Full papers and posters can be downloaded&nbsp; directly from links provided)
</b></p>
    </center>
      <ol>
        <li>
          <p align="left">Detecting, Tracking and Classifying Animals in Underwater Observatory Video<br>
          Duane R. Edgington, Danelle E. Cline, Jerome Mariette, Ishbel Kerkez<br>
          <br>
          Abstract: For oceanographic research, remotely operated underwater vehicles
          (ROVs) and underwater observatories routinely record several hours of video material every day. Manual processing of such large amounts of video has become a major bottleneck for scientific research based on this data. We have developed an automated system that detects, tracks, and classifies objects that are of potential interest for human video annotators. By pre-selecting salient targets for track initiation using a selective attention algorithm, we reduce the complexity of multi-target tracking. Then, if an object is tracked for several frames, a visual event is created and passed to a Bayesian classifier utilizing a Gaussian mixture model to determine the object class of the detected event.<br>
          <br>
          [<a href="pdf/edgington.pdf">Paper</a>] [<a href="pdf/edgington_poster.pdf">Poster</a>]&nbsp;<br>
          <br>
        </li>
        <li>
          <p align="left"> GET-based map icon Identification for Interaction with Map and Kiosks<br>
          Huiqiong Chen, Derek Reilly<br>
          <br>
          Abstract: This paper presents a GET (Generic Edge Token) based
          approach of detecting and recognizing objects by their
          shapes, and applies it to improve our ongoing work that
          considers ways of interacting with paper maps using a
          handheld. In our work, the GET-based technique aims to
          help user better locate points of interest on map by
          recognizing these icons from images/videos captured by
          handheld camera. In this method, video/image content can
          be described using a set of perceptual shape features called GETs. Perceptible object can be extracted from a GET map,
          and then be compared against pre-defined icon models
          based on GET shape features in recognition. This method
          provides a simple, efficient way to locate points of interest
          on the map, determining handheld location, orientation
          when combined with RFID (senor-based) technique. The
          tests show that the GET-based object identification can be
          executed in reasonable time for the real-time interaction
          system. Meanwhile, the detections and recognitions are
          robust under different lighting conditions, camera focus,
          camera rotation, and distance from the map.<br>
          <br>
          [<a href="pdf/chen.pdf">Paper</a>] [<a href="pdf/chen_poster.pdf">Poster</a>]&nbsp;<br>
          <br>
        </li>
        <li>
          <p align="left"> Face Recognition in Video Using Modular ARTMAP Neural Networks&nbsp;<br>
          M. Barry and E. Granger<br>
          <br>
          Abstract: In video-based of face recognition applications, the What-and-Where Fusion Neural Network (WWFNN) has been shown to reduce the generalization error by accumulating a classifier’s predictions over time, according to each individual in the environment. In this paper, three ARTMAP variants – fuzzy ARTMAP, ART-EMAP (Stage 1) and ARTMAP-IC – are compared for the classification of faces detected in the WWFNN. ART-EMAP (stage 1) and ARTMAP-IC expand on the well-known fuzzy ARTMAP by using distributed activation of category neurons, and by biasing distributed predictions according to the number of time these neurons are activated by training set patterns. The average performance of the WWFNNs with each ARTMAP network is compared to the WWFNN with a reference k-NN classifier in terms of generalization error, convergence time and compression, using a data set of real-world video sequences. Simulations results indicate that when ARTMAP-IC is used inside the WWFNN, it can achieve a generalization error that is significantly higher (about 20% on average) than if fuzzy ARTMAP or ARTEMAP is used. Indeed, ARTMAP-IC is less discriminant than the two other ARTMAP networks in cases with complex decision bounderies, when the training data is limited and unbalanced, as found in complex video data. However, ARTMAP-IC can outperform the others when classes are designed with a larger number of training patterns.<br>
          <br>
          [<a href="pdf/barry.pdf">Paper</a>] [<a href="pdf/barry_poster.pdf">Poster</a>]<br>
          <br>
        </li>
        <li>
          <p align="left">A Simple Inter- and Intrascale Statistical Model for Video Denoising in
          3-D Complex Wavelet Domain Using a Local Laplace Distribution<br>
          Hossein Rabbani, Mansur Vafadust, Saeed Gazor<br>
          <br>
          Abstract:
          This paper presents a new video denoising algorithm
          based on the modeling of wavelet coefficients in each
          subband with a Laplacian probability density function (pdf) with local variance. Since Laplacian pdf is<br>
          leptokurtic, it is able to model the sparsity of wavelet
          coefficients. We estimate the local variance of this pdf<br>
          employing adjacent coefficients at same scale and parent
          scale. This local variance models interscale dependency
          between adjacent scales and intrascale dependency
          between spatial adjacent. Within this framework, we
          design a maximum a posteriori (MAP) estimator for
          video denoising, which relies on the proposed local pdf.
          Because separate 3-D transforms, such as ordinary 3-D
          wavelet transforms, have artifacts that degrade the
          performance of this transform, we implement our
          algorithm in 3-D complex wavelet transform. This nonseparable
          and oriented transform gives a motion-based
          multiscale decomposition for video that isolates in its
          subbands motion along different directions. In addition,
          we use our denoising algorithm in 2-D complex wavelet
          transform, where the 2-D transform is applied to each
          frame individually. Despite the simplicity of our method
          in implementation, it achieves better performance than
          several denoising methods both visually and in terms of
          peak signal-to-noise ratio (PSNR).<br>
          <br>
          [<a href="pdf/rabbani.pdf">Paper</a>] [<a href="pdf/video_denoising_rabbani_fig5.avi">Video1</a>,
          <a href="video_denoising_rabbani_fig6.avi">Video2</a>]<br>
          <br>
        </li>
        <li>
          <p align="left">Automatic extraction of semantic object in image using local
          brightness variances<br>
          Chee Sun Won<br>
          <br>
          Abstract:
          This paper deals with the problem of segmenting
          semantic object in an image. Fully automatic solution
          of this problem is not possible, but human intervention
          is needed for outlining the rough boundary of the
          semantic object to be segmented. Our goal is to make
          the object extraction automatic after the first semiautomatic
          segmentation. To achieve our goal, we
          manipulate the contrast of the object and the background such that any contrast-based object
          segmentation method can extract the object automatically.&nbsp;<br>
          <br>
          [<a href="pdf/cswon.pdf">Paper</a>]&nbsp;<br>
          <br>
        </li>
        <li>
          <p align="left">Zoom on the evidence with
          ACE Surveillance (+demo)<br>
          Dmitry O. Gorodnichy, Mohammad A. Ali, Elan Dubrofsky, Kris Woodbeck<br>
          <br>
          Abstract: Despite the population's growing awareness of the need to use surveillance systems for better security in private and business settings, such systems still have not become commonplace.
          The main reason for this is the amount of time and resources an average user has to dedicate in order to
          collect video data and then to dig through it  searching for a  evidence when using traditional
          DVR-based surveillance systems. Here we present ACE-Surveillance - an automated surveillance technology based on real-time Annotation of Critical
          Evidence that provides an efficient and low-cost solution to the problem.
          We describe the main features of this technology as related to its two  components:  ACE-Capture and ACE-Browser.
          The first component deals with detection and archival of annotated evidence, which is normally performed on a client's desktop
          computer. The latter deals with browsing and displaying archived  video evidence and can be performed either locally on client's computer or  remotely via a dedicated server.
          A new  Zoom-on-the-Evidence browsing technique  featured by the ACE Surveillance is introduced.&nbsp;
          Demonstrations of running the technology on several real-life long-term monitoring assignments,
          including its deployment by the NRC commissionaires,&nbsp; are shown.<br>
          <br>
          [<a href="pdf/ace.pdf">Paper</a>] [<a href="pdf/ace_poster.pdf">Poster</a>] [<a href="http://ace.vrs.iit.nrc.ca">Link</a>]<br>
          <br>
        </li>
        <li>
          <p align="left">Working with computer hands-free using Nouse
          Perceptual Vision Interface (+demo)<br>
          Dmitry O. Gorodnichy, Elan Dubrofsky, Mohammad A. Ali<br>
          <br>
          Abstract: Normal work with a computer implies being able to perform the following three
          computer control tasks: 1) pointing , 2) clicking, and 3) typing. Many attempts have been made to make it possible to perform these tasks hands-free using a video image of the user as input.
          Nevertheless, as reported by assistive technology practitioners, no real vision-based hands-free computer control solution
          that can be used by disabled users has been produced as of yet. Here we present the Nouse Perceptual Vision Interface
          (Nouse-PVI)  that is hoped to finally offer such as a solution. Evolved from the original Nouse "Nose as Mouse" concept and
          currently under testing with SCO Health Service, Nouse-PVI  has several unique features that make it
          preferable to other hands-free computer input alternatives.
          These include i) using the nose tip, which has been confirmed to be very convenient for disabled
          users, for
          whom the nose literally becomes a new "finger" that can be used to
          control the computer; ii) a feedback-providing mechanism called
          Perceptual Cursor (Nousor) that creates an invisible link between the computer and user and which is very important for control as it allows the user to adjust his/her head motion so that the computer can better interpret
          it, and iii) a number of design solutions  specifically tailored for  vision-based data entry  using small range head motion, such as
          motion-based code entry (NouseCode and NouseTyper), a motion-based virtual keyboard (NouseBoard and
          NouseEdit) and a drawing tool
          (NouseChalk).&nbsp; While demonstrating these innovative tools, we also address the issue of the
          user's ability and readiness to work with a computer in the brand new way,
          i.e. hands-free. The problem is that a user has to understand that it is not entirely  the responsibility of
          the computer to understand what one wants, but it's also the responsibility of the user.
          - Just as a conventional  computer user cannot move the cursor on the screen without first putting
          the hand on the mouse, so a perceptual interface user cannot  work with a computer until
          s/he "connects" to it.
          That is, the computer and user must work as a team  for the best control results to be achieved.
          This presentation is therefore  designed to serve both as a guide to those  developing vision-based input  devices and as a tutorial for those who will be using them.<br>
          <br>
          [<a href="pdf/Nouse.pdf">Paper</a>] [<a href="pdf/Nouse_poster.pdf">Poster</a>]
          [<a href="http://vrs.iit.nrc.ca/Nouse">Link</a>]
        </li>
           <li>
          <p align="left">Edge Based Tracking for Traffic Surveillance<br>
          R. Zabihollahi, M. Soryani, and A. Tajbakhsh<br>
          <br>
          Abstract:This paper investigates on tracking vehicles based
on edge detection. The main application studied in this
paper is Velocity Detection which is still a challenge in
traffic surveillance. By using several samples of
normal road, a reference image is created and objects
are determined from edge detection. Some new noise
filtering techniques proposed for this method described
as well. The method tested on several real road movies
which contains known velocities. The result of method
compared with a well-known intensity based object
tracking method and comparison results presented.
          <br>
          [<a href="pdf/rz.pdf">Paper</a>]&nbsp;<br>
          <br>
        </li>

      </ol>
    <p>* Oral papers are published by IEEE Computer Society Press as part of the
    the <a href="http://http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/proceedings/crv/&amp;toc=comp/proceedings/crv/2007/2786/00/2786toc.xml">CRV'07
    Proceedings</a>.&nbsp;&nbsp; The BibTex reference of these papers is:&nbsp; <font size="1">@inproceedings{vp4s06-paper,&nbsp;&nbsp;&nbsp;title
    = &quot;... &quot;,&nbsp;&nbsp;&nbsp;author = &quot;...&quot;,&nbsp;&nbsp;&nbsp;year=&quot;2007&quot;,&nbsp;booktitle&nbsp;
    = &quot;International CRV Workshop on Video Processing and Recognition
    (VideoRec'07), in CRV Proceedings, </font>May 28-30, Montreal, Canada<font size="1">&quot;,
    pages = &quot;&quot; }<br>
    <br>
    ** Poster/Demo </font>papers are published online at the workshop website.&nbsp;The
    BibTex reference of these papers is: <font size="1">@inproceedings{vp4s06-paper,&nbsp;&nbsp;&nbsp;
    title = &quot;... &quot;,&nbsp;&nbsp;&nbsp; author = &quot;...&quot;,&nbsp;&nbsp;&nbsp;
    year=&quot;2007&quot;,&nbsp;booktitle&nbsp; = &quot;International CRV
    Workshop on Video Processing and Recognition (VideoRec'07), </font>May
    28-30, Montreal, Canada (online at&nbsp; <font size="1"><a href="http://www.computer-vision.org/VideoRec07">http://www.computer-vision.org/VideoRec07</a>)&quot;}</font><!-- End ofNews ----> 
 
    </TD></TR></TBODY></TABLE>



  <P align=center><font size="1">Last updated: 20.VII.2007.</font></P></BODY></HTML>
