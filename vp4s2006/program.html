<HTML>
<HEAD>

<TITLE>VP4S-06: First International Workshop on Video Processing for Security</TITLE>
<!-- link rel="stylesheet" href="http://www.cv.iit.nrc.ca/pics/default.css"-->
<meta http-equiv="Content-Language" content="en-ca">
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<style>
<!-
a:active { font-weight: bold; text-decoration: underline }
A:visited { font-weight: bold; color:#993300; text-decoration: none;}
A:link { font-weight: bold; color:#12522E; text-decoration: none;}
A:hover { color:#12522E; text-decoration: underline;background-color:#cccccc;}
A:border { border: 1px solid black; }
P		{font-family: Verdana, Arial, Helvetica; font-size: 8pt;};
UL		{font-family: Verdana, Arial, Helvetica; font-size: 8pt;};
OL		{font-family: Verdana, Arial, Helvetica; font-size: 8pt;};
TD		{font-family: Verdana, Arial, Helvetica; font-size: 8pt;};
DIV		{font-family: Verdana, Arial, Helvetica; font-size: 8pt;};
PRE		{font-family: Times New Roman Cyr, Times New Roman; font-size: 9pt;};
BODY		{font-family: Verdana, Arial, Helvetica; font-size: 8pt;};
INPUT		{font-family: Verdana, Arial, Helvetica; font-size: 8pt;};
TEXTAREA	{font-family: Verdana, Arial, Helvetica; font-size: 8pt;};
->
</style>


</head>


<!--BODY text=#000000 vLink=#551a8b aLink=#ff0000 link=#0000ee bgColor=#fffff4-->
<CENTER>


<TABLE cellPadding=5 width=750 bgColor=#FFFF66 noborder border="0">
  <TBODY>
  <TR>
    <TD colSpan=4 bgcolor="#FF3300">
    
    
<!---   header ------>    
      <TABLE cellSpacing=0 cellPadding=2 width="100%" border=0 bgcolor="#008080">
        <TBODY>
        <TR>
          <TD vAlign=middle align=center width=90 bgColor=#FF3300>
          <a href="http://www.visioninterface.net/fpiv04/preface.html#logo"><img border="0" src="pics/fpiv-logo.gif" 
alt="Click here to read  
  about FPiV logo" width="90" height="90"></a> </TD>
          <TD noWrap align=middle bgColor=#FF3300 height="100">
            <p align="center"><b>
            VP4S-06: The First International Workshop on&nbsp;<font face="Arial,Helvetica" size="5"><br>
            </font></b><font size="6">
 <b>
 <i>
 Video&nbsp;Processing for Security</i></b></font><br>
June 7-9, Quebec City, Canada
            <BR><a href="http://www.computer-vision.org/4security">www.computer-vision.org/4security</a></p>
          </TD>
          <TD align=middle width=90 bgColor=#FF3300 nowrap>
            <p align="center"><a href="http://www.cipprs.org"><img border="0" src="pics/cipprs2.gif" width="79" height="78"></a><a href="http://iit-iti.nrc-cnrc.gc.ca/r-d/cv-vi_e.html"><img  src="pics/NRC-CNRC.gif" border="0" width="82" height="16"></a></p>
 </TD></TR>
             
        <!-- TR>
          <TD bgColor=#ff8888 colSpan=3>
            <p align="right"><font face="Times New Roman" size="6"><i><b>FPIV
            2004</b></i>&nbsp;</font></TD></TR-->
            </TBODY>
            </TABLE>
            </TD>
            </TR>
 
   <TR>
    <td align=center width="25%" bgcolor=#FF3300 height="25" nowrap><font size="1"><b><a href="index.html">introduction</a></b></font> </td>
    <TD noWrap align=center width="25%" bgcolor=#FF3300><font size="1"><b><a href="authors.html">submission
      instructions</a></b></font> 
 </TD>   
    <TD noWrap align=center width="25%" bgcolor=#FF3300><b><font size="1"><a href="program.html">workshop
      contributions</a>&nbsp;</font></b>



    </TD>
<TD noWrap align=center width="25%" bgcolor=#FF3300> <b><a href="http://www.ift.ulaval.ca/ai06/">AI'06</a>
      / <a href="http://www.cs.usask.ca/~gutwin/gi/">GI'06</a> / <a href="http://www.computerrobotvision.org/">CRV'06</a></b>



 </TD>
      </TR>
      
 <!-- End of header ----> 
      
  <TR>
    <TD colSpan=4> 

    
        <p align="center"><b><font size="2">This workshop is held jointly
        with&nbsp;<br>
        the <a href="http://www.computerrobotvision.org/">Canadian conference
        on Computer &amp; Robot Vision (CRV'06</a>)</font></b></p>

    
        <hr>


        <table border="0" width="100%">
          <tr>
            <td width="6%" bgcolor="#FFFFFF">
              <p align="center"><b>E</b></td>
            <td width="194%" bgcolor="#FFFFFF" colspan="2">
              <p align="center"><b><a href="06-vp4s-editorial.pdf"><font size="4">Editorial
              from Workshop Chairs
              </font></a></b><hr>
              
            </td>
          </tr>
          <tr>
            <td width="6%">
              <p align="center">O</td>
            <td width="194%" colspan="2">
              <p align="center"><b><font size="4">Oral Papers</font><font size="3">*&nbsp;<br>
              </font></b><font size="2"><b>Thusday, 8 June, 2006 (15:10-17:50) - as Special Session of CRV'06</b><br>
              (Full papers can be downloaded from <a href="http://www.ieeexplore.ieee.org">ieeexplore.ieee.org</a>
              or by clicking on the paper title)</font><hr>
            </td>
          </tr>
          <tr>
            <td width="1%" bgcolor="#FFFFFF" valign="middle" align="center"><font size="2">1</font></td>
            <td width="24%" bgcolor="#FFFFFF" valign="middle" align="center"><img border="0" src="shots/o1.gif" width="142" height="108">&nbsp;
              <p>[<a href="pdf/4-moon.ppt">Talk slides</a>]&nbsp;</p>
            </td>
            <td width="170%" bgcolor="#FFFFFF"><font size="2"><a href="http://ieeexplore.ieee.org/iel5/10921/34370/01640391.pdf?tp=&amp;arnumber=1640391&amp;isnumber=34370">A Pixel-Weighting Method for Discriminating Objects of&nbsp;Different Sizes in an Image Captured from a Single Camera<br>
              </a>Mookyung Park, Namsu Moon, Sang Rim Ryu, Jeong Pyo Kong, Y.J. Lee,
              W.J. Mun (S1 Corporation, Korea)<br>
              <br>
              </font><font FACE="Times-BoldItalic" SIZE="1">Abstract - </font><font size="1">A novel method of pixel-weighting is
              proposed to calculate the size of a detected object in an image
              captured using a single camera. The calculated object size does
              not vary significantly regardless of the location of the object in
              an image, which allows it to be effectively utilized in a
              vision-based surveillance sensing algorithm as a meaningful
              feature for discriminating human intruders from other objects.
              Experimental results show the feasibility of the proposed method.</font>
              <hr>
            </td>
          </tr>
          <tr>
            <td width="1%" bgcolor="#FFFFFF" valign="middle" align="center"><font size="2">2</font></td>
            <td width="24%" bgcolor="#FFFFFF" valign="middle" align="center"><img border="0" src="shots/o2.jpg" width="159" height="116">
              <p>[<a href="pdf/4-ali.pdf">Talk slides</a>] [<a href="pdf/ali-MOV00414.MPG">Video</a>]</p>
            </td>
            <td width="170%" bgcolor="#FFFFFF"><font size="2">
          <a href="http://ieeexplore.ieee.org/iel5/10921/34370/01640392.pdf?tp=&amp;arnumber=1640392&amp;isnumber=34370">	A Novel Clustering-Based Method for Adaptive Background Segmentation<br>
          </a>S. Indupalli, M.A. Ali, B. Boufama&nbsp;(University of Windsor)</font>
              <p ALIGN="LEFT"><font FACE="Times-BoldItalic" SIZE="1">Abstract - </font><font size="1">This paper presents a new
              histogram-based method for dynamic background modeling using a
              sequence of images extracted from video. In particular, a k-means
              clustering technique has been used to identify the foreground
              objects. Because of its shadow resistance and discriminative
              properties, we have used images in the HSV color space instead of
              the traditional RGB color space. The experimental results on real
              images are very encouraging as we were able to retrieve perfect
              backgrounds in simple scenes. In very complex scenes, the
              backgrounds we have obtained were very good. Furthermore, our
              method is very fast and could be used in real-time applications
              after optimization.</font>
              <hr>
            </td>
          </tr>
          <tr>
            <td width="1%" bgcolor="#FFFFFF" valign="middle" align="center"><font size="2">3</font></td>
            <td width="24%" bgcolor="#FFFFFF" valign="middle" align="center"><img border="0" src="shots/o3.jpg" width="159" height="118"></td>
            <td width="170%" bgcolor="#FFFFFF"><font size="2"><a href="http://ieeexplore.ieee.org/iel5/10921/34370/01640393.pdf?tp=&amp;arnumber=1640393&amp;isnumber=34370">Object
              detection and tracking using iterative division and correlograms<br>
              </a>Rafik Bourezak  and Guillaume-Alexandre Bilodeau&nbsp;(Ecole
            Polytechnique de Montreal)</font>
              <p ALIGN="LEFT"><font FACE="Times-BoldItalic" SIZE="1">Abstract - </font><font size="1">This paper presents algorithms for
              the detection and tracking of moving objects and their
              relationships. The algorithms are based on color and texture
              analysis for real time processing. Our goal is to study human
              interaction by tracking people and objects for surveillance
              applications. The object detection algorithm is based on color
              histograms and iteratively divided interest regions for motion
              detection. The tracking algorithm is based on correlograms which
              combines spectral and spatial information to match detected
              objects in consecutive frames.</font>
              <hr>
            </td>
          </tr>
          <tr>
            <td width="1%" bgcolor="#FFFFFF" valign="middle" align="center"><font size="2">4</font></td>
            <td width="24%" bgcolor="#FFFFFF" valign="middle" align="center"><img border="0" src="shots/o4.gif" width="152" height="126"></td>
            <td width="170%" bgcolor="#FFFFFF"><font size="2"><a href="http://ieeexplore.ieee.org/iel5/10921/34370/01640394.pdf?tp=&amp;arnumber=1640394&amp;isnumber=34370">	Collaborative Multi-Camera Surveillance with Automated Person Detection<br>
              </a>Trevor Ahmedali and James J. Clark&nbsp;(McGill University)<br>
              <br>
              </font><font FACE="Times-BoldItalic" SIZE="1">Abstract - </font><font size="1">This paper presents the groundwork for a
              distributed&nbsp; network of collaborating, intelligent
              surveillance cameras, implemented on a set of low-cost&nbsp;&nbsp;
              embedded microprocessor- based camera modules. Each camera trains
              a person detection classifier using the Winnow algorithm for
              unsupervised, online learning. Training examples are automatically
              extracted and labelled, and the classifier is then used to locate
              any person instances. To improve detection performance, multiple
              cameras with overlapping fields of view collaborate to confirm
              results. We present a novel, unsupervised calibration technique
              that allows each camera module to efficiently understand its
              spatial relationship with the other cameras. During runtime,
              cameras apply the learned spatial correlations to confirm each
              other’s detections. This technique implicitly handles
              non-overlapping regions that cannot be confirmed. Its
              computational efficiency makes it well-suited to real-time
              processing on our hardware.</font>
              <hr>
            </td>
          </tr>
          <tr>
            <td width="1%" bgcolor="#FFFFFF" valign="middle" align="center"><font size="2">5</font></td>
            <td width="24%" bgcolor="#FFFFFF" valign="middle" align="center"><img border="0" src="shots/o5.jpg" width="155" height="118">
              <p>[<a href="pdf/4-petrushin.pdf">Talk slides</a>] [<a href="pdf/petrushin-elevator.wmv">Video</a>]</p>
            </td>
            <td width="170%" bgcolor="#FFFFFF"><font size="2"><a href="http://ieeexplore.ieee.org/iel5/10921/34370/01640395.pdf?tp=&amp;arnumber=1640395&amp;isnumber=34370"> 	A Multiple-Sensor Indoor Surveillance System<br>
              </a>Valery A. Petrushin, Omer Shakil, Damian Roqueiro, Gang Wei, Anatole V. Gershman&nbsp;(Accenture Technology)<br>
              <br>
              </font><font FACE="Times-BoldItalic" SIZE="1">Abstract - </font><font size="1">This paper describes an approach for people
              localization and tracking in an office environment using a sensor
              network that consists of video cameras, infrared tag readers, a
              fingerprint reader and a PTZ camera. The approach is based on a
              Bayesian framework that uses noisy, but redundant data from
              multiple sensor streams and incorporates it with the contextual
              and domain knowledge that is provided by both the physical
              constraints imposed by the local environment where the sensors are
              located and by the people that are involved in the surveillance
              tasks. The experimental results are presented and discussed.</font>
              <hr>
            </td>
          </tr>
          <tr>
            <td width="1%" bgcolor="#FFFFFF" valign="middle" align="center"><font size="2">6</font></td>
            <td width="24%" bgcolor="#FFFFFF" valign="middle" align="center"><img border="0" src="shots/o6.jpg" width="155" height="97">
              <p>[<a href="pdf/4-park.pdf">Talk slides</a>] [<a href="pdf/park-3d.avi">Video</a>]</p>
            </td>
            <td width="170%" bgcolor="#FFFFFF"><font size="2"><a href="http://ieeexplore.ieee.org/iel5/10921/34370/01640396.pdf?tp=&amp;arnumber=1640396&amp;isnumber=34370">	3D Face Reconstruction from Stereo Video<br>
              </a>Unsang Park, Anil K. Jain&nbsp;(Michigan State University)<br>
              <br>
              </font><font FACE="Times-BoldItalic" SIZE="1">Abstract - </font><font size="1">Face processing in video is receiving
              substantial attention due to its importance in many
              security-related applications. A video provides rich information
              about a face(multiple frames and temporal coherence) that can be
              utilized in conjunction with 3D face models, if available, to
              establish a subject’s identity. We propose a 3D face modeling
              method based on constructing a user-specific model derived from a
              generic 3D face model and stereo images (two video frames) of the
              user. The user-specific 3D face model can be used both in
              enrollment and recognition stages. The advantage of utilizing
              reconstructed 3D face model is demonstrated by conducting face
              recognition experiments for 9 probe subjects against a gallery
              database containing 100 subjects.</font>
              <hr>
            </td>
          </tr>
          <tr>
          
            <td width="1%" bgcolor="#FFFFFF" valign="middle" align="center"><font size="2">7</font></td>
          
            <td width="24%" bgcolor="#FFFFFF" valign="middle" align="center"><img border="0" src="shots/o7.jpg" width="153" height="107"></td>
            <td width="170%" bgcolor="#FFFFFF">

 
        <font size="2">
          <a href="http://ieeexplore.ieee.org/iel5/10921/34370/01640397.pdf?tp=&amp;arnumber=1640397&amp;isnumber=34370">	User Authentication based on Face Recognition with Support Vector Machines</a><br>
            Paolo Abeni, Madalina Baltatu, Rosalia D’Alessandro&nbsp;(TelecomItalia
        Lab,&nbsp;
            Italy)<br>
          <br>
        </font><font FACE="Times-BoldItalic" SIZE="1">Abstract - </font><font size="1">The present paper proposes an authentication
        scheme which relies on face biometrics and one-class Support Vector
        Machines. The proposed recognition procedures are based on both a global
        approach and on a combination of a global and a component- based
        approaches. Two different features extraction methods and three light
        compensation algorithms are tested. The combined system outperforms the
        global system and yields a significant performance enhancement with
        respect to the prior results obtained with the one-class Support Vector
        Machines approach for face recognition.</font>
        <hr>
            </td>
          </tr>
          <tr>
            <td width="6%" valign="middle" align="center"><font size="2">P</font></td>
            <td width="194%" valign="middle" align="center" colspan="2"><b><font size="4">Poster/Demo
              Papers**</font><font size="3"><br>
              </font></b><font size="2"><b>
        Friday, 9 June, 2006 (15:30-17:30) - jointly with CRV'06 poster session</b><br>
              (Full papers can be downloaded&nbsp; directly by clicking on the
              paper title)</font>
              <hr>
            </td>
          </tr>
          <tr>
            <td width="1%" bgcolor="#FFFFFF" valign="middle" align="center"><font size="2">1</font></td>
            <td width="24%" bgcolor="#FFFFFF" valign="middle" align="center"><img border="0" src="shots/p1.jpg" width="158" height="119">
              <p>[<a href="pdf/4-sherbrooke.jpg">Poster</a>]</p>
            </td>
            <td width="170%" bgcolor="#FFFFFF"><font size="2"><a href="pdf/sherbrooke.pdf">Region Competition for Object Tracking by Using Kullback-Leibler Distance and Level Set Contours</a><br>
            Mohand Saïd Allili  and Djemel Ziou (Université de Sherbrooke)<br>
            <br>
            </font><font FACE="Times-BoldItalic" SIZE="1">Abstract - </font><font size="1">In this paper, we propose a
            novel object tracking algorithm in video sequences. The formulation
            of our tracking model is based on variational calculus, where region
            and boundary information cooper-ate for object boundary localization
            by using active contours. In the approach, only the segmentation of
            the objects in the 1rst frame is required for initialization. The
            evolution of the object contours on a current frame aims to 2nd the
            boundary of the objects by minimizing the Kullback- Leibler distance
            of the region features distribution in the vicinity of the contour
            to the objects versus the background respectively. We show the
              effectiveness of the approach on examples of object tracking
            performed on real video sequences.</font>
              <hr>
            </td>
          </tr>
          <tr>
            <td width="1%" bgcolor="#FFFFFF" valign="middle" align="center"><font size="2">2</font>
            </td>
            <td width="24%" bgcolor="#FFFFFF" valign="middle" align="center"><img border="0" src="shots/p2.jpg" width="152" height="103">
              <p>[<a href="jpg/multiple-tracking.jpg">Poster</a>]</p>
            </td>
            <td width="170%" bgcolor="#FFFFFF"><font size="2"><a href="pdf/windsor.pdf">Tracking Multiple People for Video Surveillance</a><br>
            M. A. Ali, S. Indupalli and B. Boufama (University of Windsor)<br>
              <br>
            </font><font FACE="Times-BoldItalic" SIZE="1">Abstract - </font>This paper addresses the problem of detecting and&nbsp; tracking multiple moving people in a complex
              environment with unknown background. In this
              paper, we propose a new correlation-based matching
              technique for feature-based tracking. Our method was
              compared with two existing matching techniques, namely the normalized Euclidean distance and histogram-based
              matching. Experimental results on real-images
              suggest that our correlation-based approach is more
              accurate and efficient than the other two approaches.
              <hr>
            </td>
          </tr>
          <tr>
            <td width="1%" bgcolor="#FFFFFF" valign="middle" align="center"><font size="2">3</font></td>
            <td width="24%" bgcolor="#FFFFFF" valign="middle" align="center"><img border="0" src="shots/p3.jpg" width="157" height="117">
              <p>[<a href="jpg/mpeg-tracking.jpg">Poster</a>]</p>
            </td>
            <td width="170%" bgcolor="#FFFFFF"><font size="2"><a href="pdf/saskatchewan.pdf">Detection of Moving Objects in Video Scene - MPEG like Motion Vector vs. Optical Flow</a><br>
            Kunio Takaya (University of Saskatchewan)<br>
            <br>
            </font><font SIZE="1">Abstract - This paper demonstrates the use
            of motion vector as de&#12;ned in MPEG video encoder to detect and
            crudely segment moving objects in video scene. The proposed motion
            vector search algorithm incorporated a mechanism to suppress search
            in the still background and to invalidate the motion vectors found
            at search boundaries. This paper presents four cases of video clips
            processed to calculate and identify the MPEG like motion vectors
            greater than a certain magnitude and to segment out corresponding
            macro-blocks, which constitute a crude segmentation of moving
            objects. The MPEG like motion vectors drawn for a consecutive image
            pair were compared against the vector field calculated by the
            optical ow method for subjective comparison. </font>
              <hr>
            </td>
          </tr>
          <tr>
            <td width="1%" bgcolor="#FFFFFF" valign="middle" align="center"><font size="2">4</font>
            </td>
            <td width="24%" bgcolor="#FFFFFF" valign="middle" align="center"><img border="0" src="shots/p4.jpg" width="154" height="114">
              <p>[<a href="pdf/4-ACE.pdf">Poster</a>]</p>
            </td>
            <td width="170%" bgcolor="#FFFFFF">
        <font size="2"><a href="pdf/ottawa.pdf">ACE Surveillance: the next generation surveillance for  long-term  monitoring and activity summarization</a><br>
            Dmitry O. Gorodnichy (NRC-IIT)&nbsp;
            </font>
        <p ALIGN="LEFT"><font  SIZE="1">Abstract - </font><font size="1">This paper introduces a new concept for the area of
        video surveillance called Critical Evidence Snapshot. We
        show that automatic extraction and annotation of Critical Evidence Snapshots, which are defined as video
        snapshots that provide a piece of information that is both useful and new, is the key to improving the utility
        and efficiency of video surveillance systems. An
        implementation of an ACE (Annotated Critical Evidence) Surveillance system made from off-the-shelf cameras
        and a desktop is described. The results obtained on
        several real-life surveillance assignments confirm our vision for ACE Surveillance as the next generation technology
        for collecting and managing surveillance data.</font>&nbsp;</p>
              <hr>
            </td>
          </tr>
           <tr>
            <td width="1%" bgcolor="#FFFFFF" valign="middle" align="center">
              <font size="2">5</font>
            </td>
            <td width="24%" bgcolor="#FFFFFF" valign="middle" align="center">
              <img border="0" src="shots/p9.gif" width="156" height="119" >
              <p>
              [<a href="pdf/4-associative.pdf">Poster</a>]</p>
            </td>
            <td width="170%" bgcolor="#FFFFFF">
        <b><font size="2"><a href="pdf/4-associative.pdf">Associative tracking and recognition in video</a><br>
        </font></b>
        <font size="2">
            Dmitry O. Gorodnichy (NRC-IIT)&nbsp;
            </font>
        <p>Abstract - Due to  limited resolution and quality of  surveillance video, tracking and recognizing of objects in surveillance video&nbsp;
        requires techniques for accumulation of information about the object over time.
        The simplest of these techniques is histograms, which computes the distribution of pixel values over time and which is frequently used for tracking uniformly coloured objects such as faces.
        Another well-known technique  for the purpose is correlograms,  which  learns  pixel values and their spatial relationship to
        yield better discriminative power. However, this technique does not offer a complete learning-over-time solution either,&nbsp;because&nbsp;
        it updates the information about the object, which is expressed in terms of cooccurance matrices,
        using the currently observed  pixels only and ignoring the preceding learning history.
        Associative neural network based memorization can thus be seen as a next-level data accumulation technique suitable for learning
        of objects in video over time, which takes into account both the learning history and
        the spatial information about the object.&nbsp; This presentation describes how to use the <a href="http://synapse.vit.iit.nrc.ca/memory/pinn/">Open
        Source Associative Neural Network code</a> for tracking and recognition
        of objects in video. Two demos showing multiple-face tracking and
        classification in low-resolution video are shown.</p>
            </td>
          </tr>
          <tr>
            <td width="1%" bgcolor="#FFFFFF" valign="middle" align="center">
              <font size="2">6</font>
            </td>
            <td width="24%" bgcolor="#FFFFFF" valign="middle" align="center">
              <img border="0" src="shots/p8.jpg" width="143" height="113">
              <p>[<a href="pdf/4-wei.pdf">Poster]</a></p>
            </td>
            <td width="170%" bgcolor="#FFFFFF">
        <font size="2">
 <a href="pdf/binghamton.pdf">Sequence Based Face Characterization Using Factorized Feature Points</a>.&nbsp;<br>
            Xiaozhou Wei and Lijun Yin (SUNY Binghamton).&nbsp;<br>
              <br>
              </font><font SIZE="1">Abstract - In this paper, we proposed a face tracking and
              reconstruction scheme for face representation, human computer
              interaction, and the application of security. We applied a
              CDOF (Color Distribution Based Optical flow) approach to track the
              feature points in a facial video sequence. The extracted feature
              points sequence are then used as an input to derive their 3D
              coordinates using a factorization based algorithm. In combining
              with our model based expression generation approach, a facial
              expression from a 2D input can be reconstructed as a 3D output.
              The feasibility, limitation and future development of the proposed
              scheme are discussed through the experimentation.&nbsp;</font>
              <hr>
            </td>
          </tr>         
                   <tr>
            <td width="1%" bgcolor="#FFFFFF" valign="middle" align="center"><font size="2">7</font>
            </td>
            <td width="24%" bgcolor="#FFFFFF" valign="middle" align="center"><img border="0" src="shots/p5.jpg" width="157" height="126">
              <p>[<a href="jpg/surveillance-w-SOM.jpg">Poster</a>]</p>
            </td>
            <td width="170%" bgcolor="#FFFFFF">
        <font size="2"><a href="pdf/montreal.pdf"> Video-Surveillance application with Self-Organizing
            Maps</a> <a href="seniors.pdf">  </a><br>
            Mohamed Dahmane and Jean Meunier (Université de Montréal)<br>
            <br>
            </font>Abstract - In this demo, we present an
            approach for video surveillance detection of abnormal events based
            on target trajectory analysis. The methodology follows a typical
            modular form: Detection/ Tracking/ Recognition. The detection step
            is based on the color constancy principle and uses an adaptive
            background subtraction technique with a shadow elimination model.
            The target tracking involves a
            direct and inverse matrix matching process. In the recognition stage
            we consider local motion properties (flow vectors), and more global
            ones expressed by elliptic Fourier descriptors. From these temporal
            trajectory characterizations, two Kohonen maps allow to distinguish
            normal behavior from abnormal or suspicious
            ones. The classification results show a 94.6 % correct recognition
            rate with video sequences taken by a low cost webcam. The system
            runs at a 12Hz absolute minimum video
            acquisition frequency, providing essentially real-time analysis.&nbsp;
              <hr>
            </td>
          </tr>
          <tr>
            <td width="1%" bgcolor="#FFFFFF" valign="middle" align="center"><font size="2">8</font>
            </td>
            <td width="24%" bgcolor="#FFFFFF" valign="middle" align="center"><img border="0" src="shots/p6.jpg" width="156" height="114">
              <p>[<a href="jpg/fall-detection.jpg">Poster</a>]</p>
            </td>
            <td width="170%" bgcolor="#FFFFFF">
        <font size="2"><a href="pdf/montreal-fall_detection.pdf"> Fall Detection Using 3D Head Trajectory Extracted From a Single Camera Video
            Sequence</a>.<br>
            Caroline Rougier and Jean Meunier (Université de Montréal)<br>
            <br>
            </font><font size="1">Abstract — In Western societies, the population grows old, and
        we
            must think about solutions to help them to stay at home in a secure
            environment. By providing a specific analysis of people behavior,
            computer vision offers a good solution for healthcare systems, and
            particularly for fall detection. This demo will show the results of
            a new method to detect falls using a monocular camera. The main
            characteristic of this method is the use of head 3D trajectories for
            fall detection.</font>
              <hr>
            </td>
          </tr>
          <tr>
            <td width="1%" bgcolor="#FFFFFF" valign="middle" align="center">
              <font size="2">9</font>
            </td>
            <td width="24%" bgcolor="#FFFFFF" valign="middle" align="center">
              <p><img border="0" src="shots/p7.jpg" width="153" height="110"></p>
              <p>[<a href="jpg/medication.jpg">Poster</a>]</p>
            </td>
            <td width="170%" bgcolor="#FFFFFF">
        <font size="2"> <a href="pdf/montreal-seniors.pdf"> Detection of Medication Intake</a>.<br>
            Myriam Valin and Jean Meunier (Biomedical Engineering Institute,&nbsp; Université de Montréal)<br>
            <br>
            </font><font SIZE="1">Abstract - In the context of the growing proportion of seniors in the western
            world population and the efforts provided in home care services, a
            computer vision system has been developed for monitoring medication
            intake. The system can detect automatically medication intake using
            a single webcam. Person detection and tracking over the video
            sequence is done using color-based techniques while the recognition
            of the medication intake activity is performed using a multi-level
            scenario model. Experimental results in controlled conditions are
            shown.</font>
              <hr>
            </td>
          </tr>
 

        </table>
 
    
    </center>

 
    <p align="left">* Oral papers are published by IEEE Computer
    Society Press as part of the the <a href="http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/proceedings/crv/&amp;toc=comp/proceedings/crv/2005/2319/00/2319toc.xml">CRV'06
    Proceedings</a>.&nbsp;&nbsp; The BibTex reference of these papers is:&nbsp; <font size="1">@inproceedings{vp4s06-paper,&nbsp;&nbsp;&nbsp;title
    = &quot;... &quot;,&nbsp;&nbsp;&nbsp;author = &quot;...&quot;,&nbsp;&nbsp;&nbsp;year=&quot;2006&quot;,&nbsp;booktitle&nbsp;
    = &quot;First International Workshop on Video Processing for Security
    (VP4S-06) in Proceedings of Third Canadian Conference on Computer and Robot
    Vision (CRV'06), </font>June 7-9, Quebec City, Canada<font size="1">&quot;,
    pages = &quot;&quot; }<br>
    <br>
    ** Poster/Demo </font>papers are published online at the workshop website.&nbsp;The BibTex
    reference of these papers is: <font size="1">@inproceedings{vp4s06-paper,&nbsp;&nbsp;&nbsp;
    title = &quot;... &quot;,&nbsp;&nbsp;&nbsp; author = &quot;...&quot;,&nbsp;&nbsp;&nbsp;
    year=&quot;2006&quot;,&nbsp;booktitle&nbsp; = &quot;First International
    Workshop on Video Processing for Security (VP4S-06), </font>June 7-9, Quebec
    City, Canada (online at&nbsp; <font size="1"><a href="http://www.computer-vision.com/4security">www.computer-vision.com/4security</a>)}</font></p>
    <hr>

 
    </TD></TR></TBODY></TABLE>



</BODY></HTML>
